\documentclass{article}

% ready for submission
\usepackage{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}

\title{Global Convergence Newton}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Raffael Colonnello\\
  University of Basel\\
  \texttt{Raffael.Colonnello@unibas.ch} \\
  \And
  Fynn Gohlke\\
  University of Basel\\
  \texttt{Fynn.Gohlke@stud.unibas.ch} \\
  \AND
  Benedikt Heuser\\
  University of Basel\\
  \texttt{ben.heuser@unibas.ch} \\
}

\begin{document}

\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}
In this paper we consider problems of the form

\begin{equation}
  \min_{x \in \mathbb{R}^d} f(\mathbf{x})
\end{equation}

where $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a twice-differentiable function. First-order optimization methods are widely used for such problems due to their low per-iteration computational cost and their suitability for parallelization. They often suffer from slow convergence for ill-conditioned objective functions \cite{wright}.
Newton's method is a popular optimization algorithm that is commonly used to solve optimization
problems. It is a second-order optimization algorithm since it uses second-order information of the
objective function. Newton's method is known to have fast local convergence guarantees for convex
functions. However, the global convergence properties of Newton's method are still an active area of
research \cite{mishchenko2023regularized} \cite{hanzely2022damped}. In contrast to first-order methods like gradient descent, second-order methods, such as Newton's method can achieve much faster convergence when presented with ill conditioned Hessians by transferring the problem into a more isotropic optimization problem at the cost of an increase to cubic run time. Newton's method yields local quadratic convergence if $f$ is twice differentiable (or we have suitable regularity conditions), which degrade outside of the local regions, yielding up to sublinear global convergence guarantees, depending on the alogithm.

In this paper, we explore the theoretical foundations of several Newton-type methods that achieve different global convergence guarantees, compare their performance in a classification-type problem for two loss functions on four different datasets.
Finally we will propose two modifications of the algorithms to achieve an increase in runtime, by either coupling the Newton-type method with a conjugate gradient method for Hessian vector multiplication or Strassen's algorithm for fast matrix inversion.
\section{Background}
\subsection{Loss function and Datasets}
Let 
$X = 
\begin{bmatrix}
\hdots x_1^\top \hdots \\
\vdots \\
\hdots x_i^\top \hdots\\
\vdots \\
\hdots x_n^\top \hdots
\end{bmatrix}
\in \mathbb{R}^{n \times d}$ \quad be the set of data for $n$ datapoints with $d$ features, i.e. $x_i \in \mathbb{R}^d$ and labels $y^\top = \begin{bmatrix} y_1,...,y_n\end{bmatrix}$\\
For $\sigma(x) := \frac{\exp(x)}{1+\exp(x)}$ the loss functions w.r.t. weights $\omega$ are given by
\begin{align}
L_1(\omega) &= -\frac{1}{n} \sum_{i=1}^{n} \Big( y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \Big), \quad \hat{y}_i = \sigma(x_i^\top \omega) \\
L_2(\omega) &= \frac{1}{n} \sum_{i=1}^{n} \log\big(1 + \exp(-y_i x_i^\top \omega)\big) + r(\omega), \quad
r(\omega) = \lambda \sum_{j=1}^{d} \frac{\alpha \omega _j^2}{1 + \alpha \omega _j^2} \\
\end{align}
which yields the two optimization problems
\begin{align}
  \min _{\omega} L_1(\omega)\\
  \min _{\omega} L_2(\omega)
\end{align}
Remark 1: The 0-1 loss function for logistic regression is given by
\begin{align*}
  -\sum_{i=1}^N \log\left[ \mu_i^{\mathbb{I}(y_i=1)} (1-\mu_i)^{\mathbb{I}(y_i=0)} \right] 
  =  -\sum_{i=1}^N \left[ y_i \log \mu_i + (1 - y_i) \log(1-\mu_i) \right]
\end{align*}
for labels $y_i \in \{0,1\}$ \cite[Eq.~8.2--8.3]{murphy2012ml}.
If we instead use labels $\tilde{y}_i \in \{-1, +1\}$, the negative log-likelihood becomes
\begin{align*}
  \sum_{i=1}^N \log\left(1 + \exp(-\tilde{y}_i\,\mathbf{w}^T\mathbf{x}_i)\right)
\end{align*}
\cite[Eq.~8.4]{murphy2012ml}.
To ensure the loss functions correspond to the correct likelihood, the label encoding must match the loss form \cite[Sec.~8.3.1]{murphy2012ml}.\\
The corresponding gradients of $L_i$ are
\begin{align}
\nabla L_1(x) &= \frac{1}{n} X^\top (\hat{y} - y) \\
\nabla L_2(x) &= -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X\omega))\big) + \nabla r(x)
\end{align}
with $\nabla r(\omega)^\top = \lambda \left[ \frac{2\alpha \omega_1}{(1 + \alpha \omega_1^2)^2}, \ldots, \frac{2\alpha \omega_d}{(1 + \alpha \omega_d^2)^2} \right]$, where $\sigma(\cdot)$ is applied elementwise, and $\odot$ denotes the entrywise multiplication of vectors.


Differentiating again yields the Hessians
\begin{align}
\nabla^2 L_1(\omega) &= \frac{1}{n} X^\top D X\\
\nabla^2 L_2(\omega) &= \frac{1}{n} X^\top D X + \nabla^2 r(\omega), \quad
r(\omega) = \operatorname{diag}\Big(\lambda \frac{2\alpha (1 - 3\alpha \omega_j^2)}{(1 + \alpha \omega_j^2)^3}\Big)
\end{align}
where the diagonal matrix $D$ has entries
\begin{align}
D_{ii} &= \hat{y}_i (1 - \hat{y}_i)=  \sigma(-y_i x_i^\top \omega) \big(1 - \sigma(-y_i x_i^\top \omega)\big),\quad 
\end{align}
Observation:\\
Since $\log(\hat{y_i}),\log(1-\hat{y_i})$ are concave on $(0,\infty)$ it follows that $-\log(\hat{y_i}),-\log(1-\hat{y_i})$ are convex and thus $L_1$ is a linear combination of convex functions (which is again convex). Meanwhile $L_2$ is not guaranteed to be convex due to the non-convex regularization term $r(\omega)$. 

\subsection{Classic Newton's Method}

The classical origin of Newton's method is as an algorithm for finding the roots of functions. In this paper it is used to find the roots $x^{*}$ of $\nabla (f(x)) \hspace{0.1cm} s.t.\nabla(f(x^{*})) = 0$ and $x^{*}$ a local minimum of $f$. Newton's method combined with a stepsize $\eta$ uses the update rule \cite{wright}:
\begin{equation}
  \mathbf{x}_{k+1} = \mathbf{x}_k - \eta (\nabla^2 f(\mathbf{x}_k))^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

The inverse Hessian can be interpreted as transforming the gradient landscape to be more isotropic, thereby improving the conditioning of the problem. 
\subsection{Cubic Newton}
AICN gibt sich zwar als regularized method aus, kann in Wirklichkeit aber nicht umgehen die Matrix trotzdem zur Berechnung des Faktors Alpha invertieren zu m체ssen. Es k채mpft deshalb f체r singulare oder illcondiitoned matrizen mit genau denselben problemen, wie unregularisierte Methoden. Kann man das sicher nicht umgehen, dass man f체r das Alpha das Skalarprodukt invertieren muss
\subsection{Cubic Newton}

The cubic Newton method was one of the first to achieve a good complexity guarantee globally [REFERENCE TO DO: What convergence rate exactly?]. It is based on cubic regularization and uses the update rule:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + H ||\mathbf{x}_{k+1} - \mathbf{x}_{k}||\mathbf{I})^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{Levenberg and Marquardt method}

The Levenberg-Marquardt's algorithm [REFERENCE] is an early form of regularized Newton's method that modifies the Hessian. For ill conditioned (or singular) H regularization can increase the conergence (or make the problem solvable as $H + \lambda I$ is always invertible for sufficiently large $eig(H)> - \lambda, \lambda > 0$). The update rule is:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + \lambda_k \mathbf{I})^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{Regularized Newton}

In their 2023 article Michenko presents a variation of Newton's method that uses the update rule \cite{mishchenko2023regularized}:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + \sqrt{ H ||\nabla f(\mathbf{x}_k)||\mathbf{I}})^{-1} \nabla f(\mathbf{x}_k)
  \label{eq:regularized-newton}
\end{equation}

where $H > 0$ is a constant. The convergence rate of this algorithm is $\mathcal{O}(\frac{1}{k^2})$. This method uses an adaptive variant of the Levenberg-Marquardt regularization. 

\subsection{Appendix}
Remark 2:
\begin{align*}
  \sigma(z) &= \frac{1}{1 + e^{-z}} \\
  \implies \frac{d}{dz} \sigma(z) &= \frac{d}{dz} (1 + e^{-z})^{-1}
  = -(1 + e^{-z})^{-2} \cdot (-e^{-z}) = \frac{e^{-z}}{(1 + e^{-z})^2}
  = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}}
  = \sigma(z) (1 - \sigma(z))
\end{align*}
\begin{align*}
L_1(\omega) &= -\frac{1}{n} \sum_{i=1}^n \Big[\underbrace{y_i \log \hat{y}_i}_{=:A_i} + \underbrace{(1 - y_i) \log (1 - \hat{y}_i)}_{=:B_i}\Big] \\
\hat{y}_i &= \sigma(x_i^\top \omega) = \frac{1}{1 + e^{-x_i^\top \omega}} \\
\end{align*}
and applying Remark 2 to $\hat{y}$ we get, that
\begin{align*}
\frac{\partial}{\partial \omega} A_i &= \frac{\partial}{\partial \omega}\big(-y_i \log \hat{y}_i\big) = -y_i \frac{1}{\hat{y}_i} \hat{y}_i (1 - \hat{y}_i) x_i = -y_i (1 - \hat{y}_i) x_i \\
%
\frac{\partial}{\partial \omega} B_i &=\frac{\partial}{\partial \omega} \big(-(1 - y_i) \log (1 - \hat{y}_i)\big) = (1 - y_i) \frac{1}{1 - \hat{y}_i} \hat{y}_i (1 - \hat{y}_i) x_i = (1 - y_i) \hat{y}_i x_i\\
%
\frac{\partial}{\partial \omega} A+\frac{\partial}{\partial \omega} B &= -y_i (1 - \hat{y}_i) x_i + (1 - y_i) \hat{y}_i x_i 
= \big(-y_i + y_i \hat{y}_i + \hat{y}_i - y_i \hat{y}_i \big) x_i\\
&= (-y_i + \hat{y}_i) x_i = (\hat{y}_i-y_i ) x_i\\
%
\implies \nabla L_1(\omega) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial}{\partial \omega} A_i+\frac{\partial}{\partial \omega} B_i
= \frac{1}{n} \sum_{i=1}^n \big[\hat{y}_i - y_i\big] x_i = \frac{1}{n} X^\top (\hat{y} - y)
\end{align*}
For the Hessian it then follows
\begin{align*}
\nabla^2 _\omega L_1(\omega) &= \nabla_\omega \frac{1}{n} X^\top (\hat{y} - y) 
= \frac{1}{n} X^\top  
= \nabla _\omega (\hat{y}-y)
= \frac{1}{n} X^\top  \nabla _\omega \hat{y} \\
%
\frac{\partial}{\partial \omega} \big( \hat{y}_i x_i \big) 
&= \hat{y}_i (1 - \hat{y}_i) x_i x_i^\top\\
%
\implies \frac{\partial \hat{y}}{\partial \omega} &= \operatorname{diag}\bigl(\sigma(X\omega) \odot (1 - \sigma(X\omega))\bigr) X \\
%
\implies \nabla^2 L_1(\omega) &= \frac{1}{n} X^\top \operatorname{diag}(\hat{y} \odot (1 - \hat{y})) X \\
%
\implies \nabla^2 L_1(\omega) &= \frac{1}{n} X^\top D X\\
D &= \operatorname{diag}(\hat{y}_i (1 - \hat{y}_i)).
\end{align*}
For $L_2$ we have
\begin{align*}
L_2(\omega) &= \frac{1}{n} \sum_{i=1}^n \underbrace{\log \left(1 + \exp(-y_i x_i^\top \omega)\right) }_{f_i(\omega)} + \underbrace{\lambda \sum_{j=1}^d \frac{\alpha \omega_j^2}{1 + \alpha \omega_j^2}}_{r(\omega)} \\
\end{align*}
For the gradient we then get
\begin{align*}
\frac{\partial}{\partial \omega_j} r(\omega) &= 2 \lambda \alpha \frac{\omega_j}{(1 + \alpha \omega_j^2)^2}
 \implies \nabla r(\omega) = 2 \lambda \alpha \frac{\omega}{(1 + \alpha \omega^2)^2} \\
%
\nabla f_i(\omega) &= \frac{\partial}{\partial \omega} \log(1 + e^{-y_i x_i^\top \omega}) \notag \\
&= \underbrace{\frac{1}{1 + e^{y_i x_i^\top \omega}}}_{\sigma(-y_i x_i^\top \omega)} \cdot (-y_i x_i) = \sigma(-y_i x_i^\top \omega) \cdot (-y_i x_i) = -y_i x_i \, \sigma(-y_i x_i^\top \omega) \notag\\
%
\nabla f(\omega) &= -\frac{1}{n} \sum_{i=1}^n y_i x_i \sigma(-y_i x_i^\top \omega) = -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X \omega))\big) \\
\nabla L_2(\omega) &= \nabla f(\omega) + \nabla r(\omega) \notag \\
 &= -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X \omega))\big) + 2 \lambda \alpha \frac{\omega}{(1 + \alpha \omega^2)^2}
\end{align*}
For the Hessians we first observe two remarks:\\
Remark 3: By chain rule we have
\begin{align*}
z_i(\omega) &:= -y_i x_i^\top \omega \\
\implies \nabla_\omega z_i(\omega) &= -y_i x_i \\
\implies \nabla_\omega \sigma(z_i(\omega)) 
&= \sigma'(z_i(\omega)) \nabla_\omega z_i(\omega) \\
&= \sigma\bigl(-y_i x_i^\top \omega\bigr)\bigl(1 - \sigma\bigl(-y_i x_i^\top \omega\bigr)\bigr)\bigl(-y_i x_i\bigr)
\end{align*}
%
From the gradient we have
\begin{align*}
\nabla^2_\omega f(\omega) 
&= \nabla_\omega \left( -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X\omega))\big) \right) = -\frac{1}{n} X^\top \nabla_\omega \big(y \odot \sigma(-y \odot (X\omega))\big) \\
%
\end{align*}
Now notice, that
\begin{align*}
y \odot \sigma(-y \odot (X\omega)) &= \begin{pmatrix}
y_1 \sigma(-y_1 x_1^\top \omega) \\
y_2 \sigma(-y_2 x_2^\top \omega) \\
\vdots \\
y_n \sigma(-y_n x_n^\top \omega)
\end{pmatrix}
\end{align*}
and applying Remark 3 yields 
\begin{align*}
\nabla_\omega \sigma(-y_i x_i^\top \omega) 
&= \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big)(-y_i x_i) \\
%
\implies \nabla_\omega \big(y_i \, \sigma(-y_i x_i^\top \omega)\big) 
=& -\underbrace{y_i^2}_{=1 \text{ by Remark 1}} \, \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big) x_i 
=- \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big) x_i \\
%
\end{align*}
%
\begin{align*}
\implies &\nabla_\omega \big(y \odot \sigma(-y \odot (X\omega))\big) = -
\begin{pmatrix}
 \overbrace{\sigma(-y_1 x_1^\top \omega) \big(1 - \sigma(-y_1 x_1^\top \omega)\big)}^{=D_{1,1}} x_1 \\
 \vdots \\
 \underbrace{ \sigma(-y_n x_n^\top \omega) \big(1 - \sigma(-y_n x_n^\top \omega)\big)}_{D_{n,n}} x_n
\end{pmatrix}\\
&= -
\begin{bmatrix}
D_{1,1} & 0       & \cdots & 0       \\
0       & D_{2,2} & \cdots & 0       \\
\vdots  & \vdots  & \ddots & \vdots  \\
0       & 0       & \cdots & D_{n,n}
\end{bmatrix}
\,
\begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,d} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,d} \\
\vdots  & \vdots  & \ddots & \vdots  \\
x_{n,1} & x_{n,2} & \cdots & x_{n,d}
\end{bmatrix} \\
&= -
\begin{bmatrix}
D_{1,1}\,x_{1,1} & D_{1,1}\,x_{1,2} & \cdots & D_{1,1}\,x_{1,d} \\
D_{2,2}\,x_{2,1} & D_{2,2}\,x_{2,2} & \cdots & D_{2,2}\,x_{2,d} \\
\vdots           & \vdots           & \ddots & \vdots           \\
D_{n,n}\,x_{n,1} & D_{n,n}\,x_{n,2} & \cdots & D_{n,n}\,x_{n,d}
\end{bmatrix}
%% 
= -
\begin{bmatrix}
D_{1,1}\,x_1^\top \\
D_{2,2}\,x_2^\top \\
\vdots           \\
D_{n,n}\,x_n^\top
\end{bmatrix}
= - D X\\
\end{align*}
where we factored out the $x_i$ in the last step to rewrite it as matrix-vector product. Deriving the entire expression we conclude:
\begin{align*}
\nabla ^2 f(\omega) &= -\frac{1}{n} X^\top \nabla_\omega \big(y \odot \sigma(-y \odot (X\omega))\big) = \frac{1}{n} X^\top D X \\
D_{ii} &= \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big) \quad
\end{align*}
%
The hessian of the non-convex regularization term is derived by
\begin{align*}
\nabla ^2 _\omega r(\omega) &= \nabla _\omega \left(2 \lambda \alpha \frac{\omega_j}{(1 + \alpha \omega_j^2)^2} \right)\\
%
\frac{\partial^2}{\partial \omega_j^2} r(\omega) 
&= 2 \lambda \alpha \frac{\partial}{\partial \omega_j} \left( \frac{\omega_j}{(1 + \alpha \omega_j^2)^2} \right)
= 2 \lambda \alpha \frac{(1 + \alpha \omega_j^2)^2 - 4 \alpha \omega_j^2 (1 + \alpha \omega_j^2)}{(1 + \alpha \omega_j^2)^4}
= 2 \lambda \alpha \frac{1 - 3 \alpha \omega_j^2}{(1 + \alpha \omega_j^2)^3} \\
\implies & \nabla^2 r(\omega) = \operatorname{diag}\left(2 \lambda \alpha \frac{1 - 3 \alpha \omega_j^2}{(1 + \alpha \omega_j^2)^3}\right)_{j=1,\ldots,d}
\end{align*}
Combining the steps we derive the Hessian
\begin{align*}
\nabla^2 L_2(\omega) &= \nabla^2 f(\omega) + \nabla^2 r(\omega)
= \frac{1}{n} X^\top D X + \operatorname{diag}\left(2 \lambda \alpha \frac{1 - 3 \alpha \omega^2}{(1 + \alpha \omega^2)^3}\right) \\
D_{ii} &= \sigma(-y_i x_i^\top \omega) \big(1 - \sigma(-y_i x_i^\top \omega)\big)
\end{align*}



\subsection{Inexact Newton Method}
Given that Newton has cubic complexity we now outline how we aim to reduce the runtime by extending CG and MINRES methods to the Newton-type methods described in our paper. In order for the modified algorithms to inherit the convergence guarantees of the algorithms we want to approximate $p$ s.t.
$$ \| H p + \nabla f \| \leq \epsilon \hspace{0.2cm} \text{(absolute tolerance)} < \epsilon = 10^{-8} $$
Since $H_{1,2} = \nabla ^2 L_{1,2}$ are clearly symmetric (as both $X^\top D X$ and $\nabla ^2 r(x)$ are) we can apply the conjugate gradient method 
if the H is positive definite or have to fall back on MINRES if it is not pd. Positive definiteness depends on the data matrix and the regularizer curvature.
[TODO: runtime for MINRES and CG]\\
Every iteration of Vanilla Newton takes O($n^3$) per iteration because inversion of the Hessian costs O($n^3$).\\
for symmetric applying CG to newton drops the effort for conversion down to 
$$O(k\cdot n^2) = O(\sqrt{\kappa}\log \left( 1/\epsilon\right) \cdot n^2)$$ where $\kappa(H) = \frac{\lambda_{max}(H)}{\lambda_{max}(H)}$\\
Precondition with SSOR to reduce condition number.
\bibliographystyle{unsrt}
\bibliography{refs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%


\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerTODO{}
  \item Did you describe the limitations of your work?
    \answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerTODO{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerTODO{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerTODO{}
        \item Did you include complete proofs of all theoretical results?
    \answerTODO{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerTODO{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerTODO{}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerTODO{}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerTODO{}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerTODO{}
  \item Did you mention the license of the assets?
    \answerTODO{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerTODO{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerTODO{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerTODO{}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerTODO{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerTODO{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerTODO{}
\end{enumerate}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix


\section{Appendix}


Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.


\end{document}
