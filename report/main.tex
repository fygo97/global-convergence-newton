\documentclass{article}

% ready for submission
\usepackage{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}

\title{Global Convergence Newton}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Raffael Colonnello\\
  University of Basel\\
  \texttt{Raffael.Colonnello@unibas.ch} \\
  \And
  Fynn Gohlke\\
  University of Basel\\
  \texttt{Fynn.Gohlke@stud.unibas.ch} \\
  \AND
  Benedikt Heuser\\
  University of Basel\\
  \texttt{ben.heuser@unibas.ch} \\
}

\begin{document}

\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}
In this paper we consider problems of the form

\begin{equation}
  \min_{x \in \mathbb{R}^d} f(\mathbf{x})
\end{equation}

where $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a twice-differentiable function. First-order optimization methods are widely used for such problems due to their low per-iteration computational cost and their suitability for parallelization. They often suffer from slow convergence for ill-conditioned objective functions \cite{wright}.
Newton's method is a popular optimization algorithm that is commonly used to solve optimization
problems. It is a second-order optimization algorithm since it uses second-order information of the
objective function. Newton's method is known to have fast local convergence guarantees for convex
functions. However, the global convergence properties of Newton's method are still an active area of
research \cite{mishchenko2023regularized} \cite{hanzely2022damped}. In contrast to first-order methods like gradient descent, second-order methods, such as Newton's method can achieve much faster convergence when presented with ill conditioned Hessians by transferring the problem into a more isotropic optimization problem at the cost of an increase to cubic run time. Newton's method yields local quadratic convergence if $f$ is twice differentiable (or we have suitable regularity conditions), which degrade outside of the local regions, yielding up to sublinear global convergence guarantees, depending on the alogithm.

In this paper, we explore the theoretical foundations of several Newton-type methods that achieve different global convergence guarantees, compare their performance in a classification-type problem for two loss functions on four different datasets.
Finally we will propose two modifications of the algorithms to achieve an increase in runtime, by either coupling the Newton-type method with a conjugate gradient method for Hessian vector multiplication or Strassen's algorithm for fast matrix inversion.
\section{Background}
\subsection{Loss function and Datasets}
Let 
$X = 
\begin{bmatrix}
\hdots x_1^\top \hdots \\
\vdots \\
\hdots x_i^\top \hdots\\
\vdots \\
\hdots x_n^\top \hdots
\end{bmatrix}
\in \mathbb{R}^{n \times d}$ \quad be the set of data for $n$ datapoints with $d$ features, i.e. $x_i \in \mathbb{R}^d$ and labels $y^\top = \begin{bmatrix} y_1,...,y_n\end{bmatrix}$\\
For $\sigma(x) := \frac{\exp(x)}{1+\exp(x)}$ the loss functions w.r.t. weights $\omega$ are given by
\begin{align}
L_1(\omega) &= -\frac{1}{n} \sum_{i=1}^{n} \Big( y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \Big), \quad \hat{y}_i = \sigma(x_i^\top \omega) \\
L_2(\omega) &= \frac{1}{n} \sum_{i=1}^{n} \log\big(1 + \exp(-y_i x_i^\top \omega)\big) + r(\omega), \quad
r(\omega) = \lambda \sum_{j=1}^{d} \frac{\alpha \omega _j^2}{1 + \alpha \omega _j^2}
\end{align}
which yields the two optimization problems
\begin{align}
  \min _{\omega} L_1(\omega)\\
  \min _{\omega} L_2(\omega)
\end{align}
\textit{Remark 1: The 0-1 loss function for logistic regression is given by
\begin{align*}
  -\sum_{i=1}^N \log\left[ \mu_i^{\mathbb{I}(y_i=1)} (1-\mu_i)^{\mathbb{I}(y_i=0)} \right] 
  =  -\sum_{i=1}^N \left[ y_i \log \mu_i + (1 - y_i) \log(1-\mu_i) \right]
\end{align*}
for labels $y_i \in \{0,1\}$ \cite[Eq.~8.2--8.3]{murphy2012ml}.
If we instead use labels $\tilde{y}_i \in \{-1, +1\}$, the negative log-likelihood becomes
\begin{align*}
  \sum_{i=1}^N \log\left(1 + \exp(-\tilde{y}_i\,\mathbf{w}^T\mathbf{x}_i)\right)
\end{align*}
\cite[Eq.~8.4]{murphy2012ml}.
To ensure the loss functions correspond to the correct likelihood, the label encoding must match the loss form \cite[Sec.~8.3.1]{murphy2012ml}. Consequently labels were adapted conditioned to meet the loss functions requirements.}\\
\\
The corresponding gradients of $L_i$ are
\begin{align}
\nabla L_1(x) &= \frac{1}{n} X^\top (\hat{y} - y) \\
\nabla L_2(x) &= -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X\omega))\big) + \nabla r(x)
\end{align}
with $\nabla r(\omega)^\top = \lambda \left[ \frac{2\alpha \omega_1}{(1 + \alpha \omega_1^2)^2}, \ldots, \frac{2\alpha \omega_d}{(1 + \alpha \omega_d^2)^2} \right]$, where $\sigma(\cdot)$ is applied elementwise, and $\odot$ denotes the entrywise multiplication of vectors.


Differentiating again yields the Hessians
\begin{align}
\nabla^2 L_1(\omega) &= \frac{1}{n} X^\top D (\omega) X\\
\nabla^2 L_2(\omega) &= \frac{1}{n} X^\top D(\omega) X + \nabla^2 r(\omega), \quad
\nabla ^2 r(\omega) = \operatorname{diag}\Big(\lambda \frac{2\alpha (1 - 3\alpha \omega_j^2)}{(1 + \alpha \omega_j^2)^3}\Big)
\end{align}
where the diagonal matrix $D(\omega)$ has entries
\begin{align}
D_{ii}(\omega) &= \hat{y}_i (1 - \hat{y}_i)=  \sigma(-y_i x_i^\top \omega) \big(1 - \sigma(-y_i x_i^\top \omega)\big),\quad 
\end{align}
In order to discuss the Algorithms assumptions and conditions in the later sections of this paper we will first state a few properties of the given problems.
%
\subsection{Differentiability}
Both $L_1$ and $L_2$ satisfy $L_1, L_2 \in C ^\infty (\Omega)$ since both functions are compositions of functions from $C ^\infty (\Omega)$.
0. Symmetry of Hessian:
It is easy to verify that the Hessians of both loss functions are symmetric as
$$ (X^\top D X) \top = (X^\top D^\top (X ^\top)^\top)  = X^\top D X$$
and for $\nabla^2 L_2 $ symmetry is even more trivial as we only add a diagonal matrix $\nabla ^2 r(\omega)$ onto the first one.\\
%
\subsection{Invertibility}
The matrix $ \nabla ^2 L_1(\omega) = X^\top D(\omega)X$ is invertible if and only if X has full rank.\\
\textit{Proof:}
"$ \Longrightarrow$" (by contrapositive)
Assume that $X$ doesnt have full rank, then by the definition of rank it holds, that
$$ \exists y \neq 0 \quad s.t. \quad Xy = 0 \implies X^\top DX\underbrace{Xy}_{=0} =0 $$
$$\implies X^\top DX \quad \text{rank deficient} \implies X^\top DX \text{ not invertible} $$
"$\Longleftarrow$" Assume $X$ has full rank and let $y\neq 0$. Then $ Xy \neq 0 \quad \forall y \neq 0$
$$ \implies \underbrace{(Xy)^\top}_{\neq 0 \text{ as full rank}} D (Xy) >0 \implies X^\top D X \quad \text{is pd} \implies X^\top DX \quad \text{invertible because}$$
$\text{for positive definite (square) matrices M it holds}$
$$det(M) = det (U\Lambda U^\top) = det(U) det(M) det(U^T) = \underbrace{det(\underbrace{U U^\top}_{=I})}_{=1}  \underbrace{det(M)}_{> 0} > 0$$
From the four available datasets we selected \texttt{a9a}, \texttt{ijcnn1} and \texttt{covtype} and using the described criterion we proved, that the Hessian of $L_1$ is invertible for \texttt{ijcnn1} but not for {ijcnn1} and \texttt{covtype}. Since for singular Hessians Newton's method fails due to the necessary inversion of the Hessian during the update step it is necessary to pick an algorithm with a sufficient regularization for the respective datasets with $L_1$.\\
%
For the matrix $\nabla ^2 L_2 (\omega) = X^\top D(\omega)X + \nabla ^2 r(\omega) $ we will show, that for finite weights there exists a sufficient choice of $\alpha$ s.t. $D \succ 0 $ (positive defnite (pd)) holds.\\
$Proof: \text{Assume for now, that $\nabla ^2 r(\omega)$ is pd (we will show this at the end of this proof) for some sufficient}\\
\text{choice of alpha w.r.t. finite weights }|w_j|< \infty.\\
\text{We first observe, that $ X^\top D X$ is positive semi-definite (psd), i.e. } y^\top X^\top D \overbrace{Xy}^{= \xi} =  \xi^\top D \xi \geq 0 $
\text{because D is pd which implies that $y^\top D y > 0 \quad \forall y \neq 0$ and combining this with the observation, that $\xi = Xy $ could be equal to $0$ the inequality becomes sharp. This implies}
$$y^\top (X^\top DX + \nabla ^2 r(\omega))y = \underbrace{y^\top X^\top DXy}_{\geq 0} + \underbrace{y^\top \nabla ^2 r(\omega)y}_{>0 } > 0 \implies \nabla ^2 L_2 (\omega) \quad \text{pd} \implies \text{invertible}$$
Inspecting the Hessian of the non convex regularizer $\nabla ^2 r(\omega) = \operatorname{diag}\Big(\lambda \frac{2\alpha (1 - 3\alpha \omega_j^2)}{(1 + \alpha \omega_j^2)^3}\Big)$
of the cross entropy loss function $L_2$ we directly notice, that since the matrix is psd for $\lambda > 0, \alpha > 0$, it is invertible if and only if it is pd (because if it's not pd it means that one of its eigenvalues must be 0).
%
The matrix is diagonal and inspecting it's entries we directly see that pd holds if and onl yif $1-3\alpha\cdot w_j^2>0 $ is satisfied, which is true for $\alpha < \frac{1}{3w_j ^2}$. Thus we can always find a feasible choice for $\alpha>0$ s.t $\nabla ^2 r(\omega)$ is pd for finite weights. 
In our experiments we are presented with two practical issues, that weaken this statement. First we were given what we understood to be a mandatory parameter choice of  $\alpha = 1$ in the project description. The more interesting observation however, is that even when allowed to choose the regularization parameter $\alpha>0$ freely, the machine precision will treat any weight entries above the machine precision number as infinite and thus even though D is analytically pd we have that numerically the matrix degenerates for large weights. Numerically this can be stabilized by bounding weights heuristically, but since we focused on comparing the performance of different Newton-type methods for practical problems we refrained from doing so to not bias the results. Therefore we cannot guarantee, that the Hessian of our second loss function $L_2$ is invertible. In the experiments we will see that in fact singular Hessians appear for this Loss functions, making it intractable to solve with non-regularized Newton-type methods.\\
%
\subsection{Positive semidefiniteness of Hessian}
In the previous section we proved, that $\nabla ^2 L_1$ is psd while $L_2$ does not necessarily have this property due to possibly negative eigenvalues of the non-convex regularization term $\nabla ^2 r(\omega)$.

\subsection{Positive definiteness of Hessian}
Since $\nabla^2 L_1(\omega)$ is psd we know, that the Hessian is pd for a dataset, if and only if it is invertible (because psd hessians have non-negative eigenvalues and if they are invertible all eigenvalues are non-zero (meaning they only have positive eigenvalues and thus they're pd). It follows that the Hessian of \texttt{ijcnn1} is pd while the Hessians of \texttt{a9a} and \texttt{covtype} are not pd. Since the Hessian of the regularization term $\nabla ^2 r(\omega)$ potentially has negative diagonal entries $\nabla^2 L_2(\omega)$ is not guaranteed to pd. [TODO: I could analyze when exactly that happens, but it feels pretty useless tbh, as the point was made I wanted to make i.e. that we cannot assume pd for AICN].

\subsection{Convexity}
We know that a twice differentiable function $f$ is convex if and only if its Hessian is psd. After our previous observations we conclude, that $L_1$ is convex, while $L_2$ is not.

\subsection{Hessian Lipschitz}
Both Hessians are Lipschitz, as $ \frac{1}{n} X ^\top D X =:M$ satisfies
$$ \| M(\omega _1)-M(\omega _2)\| = \frac{1}{n}\| X^\top (D(\omega_1)-D(\omega_2))X \| \leq \underbrace{\frac{1}{n} \|X^\top \| \|X\| }_{=:C}\|D(\omega_1)-D(\omega_2)\|$$
Now consider that $d(\sigma) := \sigma(z_k)(1- \sigma(z_k))$ where $ z_k = y_i x_i^\top \omega_k$ and observe, that $\frac{d}{d\sigma} = 1-2\sigma$ for $\sigma \in (0,1)$ then it follows, that $\sigma '(z) = \sigma (z) (1-\sigma(z)$ (by Remark 2) and by mean value theorem (MVT) we can conclude, that for
\[
d'(z) = \sigma(z)(1 - \sigma(z))(1 - 2\sigma(z))
\]
we have 
\[
|d(z_1) - d(z_2)| \leq \sup_z |d'(z)| \cdot |z_1 - z_2| \quad \text{where}
\]
\[
|z_1 - z_2| = | x_i^\top (\omega_1 - \omega_2)| \leq \underbrace{|y_i|}_{\leq 1 \quad Remark 1} \|x_i\| \|\omega_1 - \omega_2\|
\]
\[
\implies |D_{ii}(\omega_1) - D_{ii}(\omega_2)| \leq \sup_z |d'(z)| \cdot \|x_i\| \|\omega_1 - \omega_2\|
\]
\[
\|D(\omega_1) - D(\omega_2)\| \leq \sup_z |d'(z)| \cdot \max_i \|x_i\| \|\omega_1 - \omega_2\|
\]
and since we have
\[
\sup_z |d'(z)| = \max _{\sigma \in (0,1)} |\sigma (1-\sigma)(1-2 \sigma)|
\]
which takes its maximum at $\sigma^* = \frac{1}{2} \pm \frac{1}{2\sqrt{3}}$ and yields $d(\sigma ^*) = \frac{1}{4}$ we conclude
\[
\|D(\omega_1) - D(\omega_2)\| \leq \underbrace{\frac{1}{4} \max_i \|x_i\|}_{=:L'} \|\omega_1 - \omega_2\|
\]
$$\implies \| M(\omega _1)-M(\omega _2)\| \leq C \|D(\omega_1)-D(\omega_2)\| \leq \underbrace{C L'}_{=:L} \|\omega_1 - \omega_2 \| $$
Since the gradient of the regularization term is also clearly bounded it follows, that $L_2$ is Lipschitz aswell, as it is the sum of Lipschitz functions.
%
\subsection{$L_{semi}$ semi-strongly self-concordance}
Referring to \cite{hanzely2022damped} for the respective definitions of the norms and the statement itself (Definition 3 in \cite{hanzely2022damped}) we notice, that semi-strongly self-concordance (sssc) implicitly assumes the invertibility of the Hessian. Since we know that $L_2$ is not convex and not guaranteed to be invertible (because the matrix can become singular for certain choices of weights) $L_2$ is not sssc. Using the same logic for invertibility of the Hessian for $L_1$ it follows that this loss function is not sssc for the datasets \texttt{a9a} and \texttt{covtype}. For \texttt{ijcnn1} we can prove that sssc holds. [TODO: My asslong proof for sssc of L1 for that dataset.]





\subsection{Classic Newton's Method}

The classical origin of Newton's method is as an algorithm for finding the roots of functions. In this paper it is used to find the roots $x^{*}$ of $\nabla (f(x)) \hspace{0.1cm} s.t.\nabla(f(x^{*})) = 0$ and $x^{*}$ a local minimum of $f$. Newton's method combined with a stepsize $\eta$ uses the update rule \cite{wright}:
\begin{equation}
  \mathbf{x}_{k+1} = \mathbf{x}_k - (\nabla^2 f(\mathbf{x}_k))^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

The inverse Hessian can be interpreted as transforming the gradient landscape to be more isotropic, thereby improving the conditioning of the problem. 
\subsection{Cubic Newton}
AICN gibt sich zwar als regularized method aus, kann in Wirklichkeit aber nicht umgehen die Matrix trotzdem zur Berechnung des Faktors Alpha invertieren zu müssen. Es kämpft deshalb für singulare oder illcondiitoned matrizen mit genau denselben problemen, wie unregularisierte Methoden. Kann man das sicher nicht umgehen, dass man für das Alpha das Skalarprodukt invertieren muss
\subsection{Cubic Newton}

The cubic Newton method was one of the first to achieve a good complexity guarantee globally [REFERENCE TO DO: What convergence rate exactly?]. It is based on cubic regularization and uses the update rule:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + H ||\mathbf{x}_{k+1} - \mathbf{x}_{k}||\mathbf{I})^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{Levenberg and Marquardt method}

The Levenberg-Marquardt's algorithm [REFERENCE] is an early form of regularized Newton's method that modifies the Hessian. For ill conditioned (or singular) H regularization can increase the conergence (or make the problem solvable as $H + \lambda I$ is always invertible for sufficiently large $eig(H)> - \lambda, \lambda > 0$). The update rule is:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + \lambda_k \mathbf{I})^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{Regularized Newton}

In their 2023 article Michenko presents a variation of Newton's method that uses the update rule \cite{mishchenko2023regularized}:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_{k} - ( \nabla^2 f(\mathbf{x}_k) + \sqrt{ H ||\nabla f(\mathbf{x}_k)||\mathbf{I}})^{-1} \nabla f(\mathbf{x}_k)
  \label{eq:regularized-newton}
\end{equation}

where $H > 0$ is a constant. The convergence rate of this algorithm is $\mathcal{O}(\frac{1}{k^2})$. This method uses an adaptive variant of the Levenberg-Marquardt regularization. 

\subsection{Appendix}
Remark 2:
\begin{align*}
  \sigma(z) &= \frac{1}{1 + e^{-z}} \\
  \implies \frac{d}{dz} \sigma(z) &= \frac{d}{dz} (1 + e^{-z})^{-1}
  = -(1 + e^{-z})^{-2} \cdot (-e^{-z}) = \frac{e^{-z}}{(1 + e^{-z})^2}
  = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}}
  = \sigma(z) (1 - \sigma(z))
\end{align*}
\begin{align*}
L_1(\omega) &= -\frac{1}{n} \sum_{i=1}^n \Big[\underbrace{y_i \log \hat{y}_i}_{=:A_i} + \underbrace{(1 - y_i) \log (1 - \hat{y}_i)}_{=:B_i}\Big] \\
\hat{y}_i &= \sigma(x_i^\top \omega) = \frac{1}{1 + e^{-x_i^\top \omega}} \\
\end{align*}
and applying Remark 2 to $\hat{y}$ we get, that
\begin{align*}
\frac{\partial}{\partial \omega} A_i &= \frac{\partial}{\partial \omega}\big(-y_i \log \hat{y}_i\big) = -y_i \frac{1}{\hat{y}_i} \hat{y}_i (1 - \hat{y}_i) x_i = -y_i (1 - \hat{y}_i) x_i \\
%
\frac{\partial}{\partial \omega} B_i &=\frac{\partial}{\partial \omega} \big(-(1 - y_i) \log (1 - \hat{y}_i)\big) = (1 - y_i) \frac{1}{1 - \hat{y}_i} \hat{y}_i (1 - \hat{y}_i) x_i = (1 - y_i) \hat{y}_i x_i\\
%
\frac{\partial}{\partial \omega} A+\frac{\partial}{\partial \omega} B &= -y_i (1 - \hat{y}_i) x_i + (1 - y_i) \hat{y}_i x_i 
= \big(-y_i + y_i \hat{y}_i + \hat{y}_i - y_i \hat{y}_i \big) x_i\\
&= (-y_i + \hat{y}_i) x_i = (\hat{y}_i-y_i ) x_i\\
%
\implies \nabla L_1(\omega) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial}{\partial \omega} A_i+\frac{\partial}{\partial \omega} B_i
= \frac{1}{n} \sum_{i=1}^n \big[\hat{y}_i - y_i\big] x_i = \frac{1}{n} X^\top (\hat{y} - y)
\end{align*}
For the Hessian it then follows
\begin{align*}
\nabla^2 _\omega L_1(\omega) &= \nabla_\omega \frac{1}{n} X^\top (\hat{y} - y) 
= \frac{1}{n} X^\top  
= \nabla _\omega (\hat{y}-y)
= \frac{1}{n} X^\top  \nabla _\omega \hat{y} \\
%
\frac{\partial}{\partial \omega} \big( \hat{y}_i x_i \big) 
&= \hat{y}_i (1 - \hat{y}_i) x_i x_i^\top\\
%
\implies \frac{\partial \hat{y}}{\partial \omega} &= \operatorname{diag}\bigl(\sigma(X\omega) \odot (1 - \sigma(X\omega))\bigr) X \\
%
\implies \nabla^2 L_1(\omega) &= \frac{1}{n} X^\top \operatorname{diag}(\hat{y} \odot (1 - \hat{y})) X \\
%
\implies \nabla^2 L_1(\omega) &= \frac{1}{n} X^\top D X\\
D &= \operatorname{diag}(\hat{y}_i (1 - \hat{y}_i)).
\end{align*}
For $L_2$ we have
\begin{align*}
L_2(\omega) &= \frac{1}{n} \sum_{i=1}^n \underbrace{\log \left(1 + \exp(-y_i x_i^\top \omega)\right) }_{f_i(\omega)} + \underbrace{\lambda \sum_{j=1}^d \frac{\alpha \omega_j^2}{1 + \alpha \omega_j^2}}_{r(\omega)} \\
\end{align*}
For the gradient we then get
\begin{align*}
\frac{\partial}{\partial \omega_j} r(\omega) &= 2 \lambda \alpha \frac{\omega_j}{(1 + \alpha \omega_j^2)^2}
 \implies \nabla r(\omega) = 2 \lambda \alpha \frac{\omega}{(1 + \alpha \omega^2)^2} \\
%
\nabla f_i(\omega) &= \frac{\partial}{\partial \omega} \log(1 + e^{-y_i x_i^\top \omega}) \notag \\
&= \underbrace{\frac{1}{1 + e^{y_i x_i^\top \omega}}}_{\sigma(-y_i x_i^\top \omega)} \cdot (-y_i x_i) = \sigma(-y_i x_i^\top \omega) \cdot (-y_i x_i) = -y_i x_i \, \sigma(-y_i x_i^\top \omega) \notag\\
%
\nabla f(\omega) &= -\frac{1}{n} \sum_{i=1}^n y_i x_i \sigma(-y_i x_i^\top \omega) = -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X \omega))\big) \\
\nabla L_2(\omega) &= \nabla f(\omega) + \nabla r(\omega) \notag \\
 &= -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X \omega))\big) + 2 \lambda \alpha \frac{\omega}{(1 + \alpha \omega^2)^2}
\end{align*}
For the Hessians we first observe two remarks:\\
Remark 3: By chain rule we have
\begin{align*}
z_i(\omega) &:= -y_i x_i^\top \omega \\
\implies \nabla_\omega z_i(\omega) &= -y_i x_i \\
\implies \nabla_\omega \sigma(z_i(\omega)) 
&= \sigma'(z_i(\omega)) \nabla_\omega z_i(\omega) \\
&= \sigma\bigl(-y_i x_i^\top \omega\bigr)\bigl(1 - \sigma\bigl(-y_i x_i^\top \omega\bigr)\bigr)\bigl(-y_i x_i\bigr)
\end{align*}
%
From the gradient we have
\begin{align*}
\nabla^2_\omega f(\omega) 
&= \nabla_\omega \left( -\frac{1}{n} X^\top \big(y \odot \sigma(-y \odot (X\omega))\big) \right) = -\frac{1}{n} X^\top \nabla_\omega \big(y \odot \sigma(-y \odot (X\omega))\big) \\
%
\end{align*}
Now notice, that
\begin{align*}
y \odot \sigma(-y \odot (X\omega)) &= \begin{pmatrix}
y_1 \sigma(-y_1 x_1^\top \omega) \\
y_2 \sigma(-y_2 x_2^\top \omega) \\
\vdots \\
y_n \sigma(-y_n x_n^\top \omega)
\end{pmatrix}
\end{align*}
and applying Remark 3 yields 
\begin{align*}
\nabla_\omega \sigma(-y_i x_i^\top \omega) 
&= \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big)(-y_i x_i) \\
%
\implies \nabla_\omega \big(y_i \, \sigma(-y_i x_i^\top \omega)\big) 
=& -\underbrace{y_i^2}_{=1 \text{ by Remark 1}} \, \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big) x_i 
=- \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big) x_i \\
%
\end{align*}
%
\begin{align*}
\implies &\nabla_\omega \big(y \odot \sigma(-y \odot (X\omega))\big) = -
\begin{pmatrix}
 \overbrace{\sigma(-y_1 x_1^\top \omega) \big(1 - \sigma(-y_1 x_1^\top \omega)\big)}^{=D_{1,1}} x_1 \\
 \vdots \\
 \underbrace{ \sigma(-y_n x_n^\top \omega) \big(1 - \sigma(-y_n x_n^\top \omega)\big)}_{D_{n,n}} x_n
\end{pmatrix}\\
&= -
\begin{bmatrix}
D_{1,1} & 0       & \cdots & 0       \\
0       & D_{2,2} & \cdots & 0       \\
\vdots  & \vdots  & \ddots & \vdots  \\
0       & 0       & \cdots & D_{n,n}
\end{bmatrix}
\,
\begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,d} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,d} \\
\vdots  & \vdots  & \ddots & \vdots  \\
x_{n,1} & x_{n,2} & \cdots & x_{n,d}
\end{bmatrix} \\
&= -
\begin{bmatrix}
D_{1,1}\,x_{1,1} & D_{1,1}\,x_{1,2} & \cdots & D_{1,1}\,x_{1,d} \\
D_{2,2}\,x_{2,1} & D_{2,2}\,x_{2,2} & \cdots & D_{2,2}\,x_{2,d} \\
\vdots           & \vdots           & \ddots & \vdots           \\
D_{n,n}\,x_{n,1} & D_{n,n}\,x_{n,2} & \cdots & D_{n,n}\,x_{n,d}
\end{bmatrix}
%% 
= -
\begin{bmatrix}
D_{1,1}\,x_1^\top \\
D_{2,2}\,x_2^\top \\
\vdots           \\
D_{n,n}\,x_n^\top
\end{bmatrix}
= - D X\\
\end{align*}
where we factored out the $x_i$ in the last step to rewrite it as matrix-vector product. Deriving the entire expression we conclude:
\begin{align*}
\nabla ^2 f(\omega) &= -\frac{1}{n} X^\top \nabla_\omega \big(y \odot \sigma(-y \odot (X\omega))\big) = \frac{1}{n} X^\top D X \\
D_{ii} &= \sigma(-y_i x_i^\top \omega)\big(1 - \sigma(-y_i x_i^\top \omega)\big) \quad
\end{align*}
%
The hessian of the non-convex regularization term is derived by
\begin{align*}
\nabla ^2 _\omega r(\omega) &= \nabla _\omega \left(2 \lambda \alpha \frac{\omega_j}{(1 + \alpha \omega_j^2)^2} \right)\\
%
\frac{\partial^2}{\partial \omega_j^2} r(\omega) 
&= 2 \lambda \alpha \frac{\partial}{\partial \omega_j} \left( \frac{\omega_j}{(1 + \alpha \omega_j^2)^2} \right)
= 2 \lambda \alpha \frac{(1 + \alpha \omega_j^2)^2 - 4 \alpha \omega_j^2 (1 + \alpha \omega_j^2)}{(1 + \alpha \omega_j^2)^4}
= 2 \lambda \alpha \frac{1 - 3 \alpha \omega_j^2}{(1 + \alpha \omega_j^2)^3} \\
\implies & \nabla^2 r(\omega) = \operatorname{diag}\left(2 \lambda \alpha \frac{1 - 3 \alpha \omega_j^2}{(1 + \alpha \omega_j^2)^3}\right)_{j=1,\ldots,d}
\end{align*}
Combining the steps we derive the Hessian
\begin{align*}
\nabla^2 L_2(\omega) &= \nabla^2 f(\omega) + \nabla^2 r(\omega)
= \frac{1}{n} X^\top D X + \operatorname{diag}\left(2 \lambda \alpha \frac{1 - 3 \alpha \omega^2}{(1 + \alpha \omega^2)^3}\right) \\
D_{ii} &= \sigma(-y_i x_i^\top \omega) \big(1 - \sigma(-y_i x_i^\top \omega)\big)
\end{align*}



\subsection{Inexact Newton Method}
Given that Newton has cubic complexity we now outline how we aim to reduce the runtime by extending CG and MINRES methods to the Newton-type methods described in our paper. In order for the modified algorithms to inherit the convergence guarantees of the algorithms we want to approximate $p$ s.t.
$$ \| H p + \nabla f \| \leq \epsilon \hspace{0.2cm} \text{(absolute tolerance)} < \epsilon = 10^{-8} $$
Since $H_{1,2} = \nabla ^2 L_{1,2}$ are clearly symmetric (as both $X^\top D X$ and $\nabla ^2 r(x)$ are) we can apply the conjugate gradient method 
if the H is positive definite or have to fall back on MINRES if it is not pd. Positive definiteness depends on the data matrix and the regularizer curvature.
[TODO: runtime for MINRES and CG]\\
Every iteration of Vanilla Newton takes O($n^3$) per iteration because inversion of the Hessian costs O($n^3$).\\
for symmetric applying CG to newton drops the effort for conversion down to 
$$O(k\cdot n^2) = O(\sqrt{\kappa}\log \left( 1/\epsilon\right) \cdot n^2)$$ where $\kappa(H) = \frac{\lambda_{max}(H)}{\lambda_{max}(H)}$\\
Precondition with SSOR to reduce condition number.
\bibliographystyle{unsrt}
\bibliography{refs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%


\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerTODO{}
  \item Did you describe the limitations of your work?
    \answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerTODO{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerTODO{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerTODO{}
        \item Did you include complete proofs of all theoretical results?
    \answerTODO{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerTODO{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerTODO{}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerTODO{}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerTODO{}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerTODO{}
  \item Did you mention the license of the assets?
    \answerTODO{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerTODO{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerTODO{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerTODO{}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerTODO{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerTODO{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerTODO{}
\end{enumerate}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix


\section{Appendix}


Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.


\end{document}
